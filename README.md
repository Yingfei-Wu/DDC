# Dual-Decoder Collaborative Learning with Multi-Hybrid View Augmentation for Self-Supervised 3D Action Recognition
![Framework](https://github.com/user-attachments/assets/895f45ac-09b0-4553-8dc4-ba4ddaf3a212)


## Abstract
Self-supervised methods, including contrastive learning and masked skeleton modeling, have demonstrated considerable potential in the field of skeleton-based action recognition. While contrastive learning captures fine-grained details at the instance level, masked skeleton modeling emphasizes joint-level features, recent studies have begun to combine these two approaches. However, existing combination methods primarily focus on integrating the tasks within the skeleton space. Moreover, existing contrastive learning methods often fail to exploit the comprehensive interaction information in skeletal structures, resulting in suboptimal performance when recognizing actions involving multiple individuals. To overcome these limitations, we introduce the Dual-Decoder Collaborative Learning with Multi-Hybrid View Augmentation (DDC) method, which connects these two tasks across multiple spaces. Specifically, the masked skeleton modeling task provides diverse views for the contrastive learning task in the skeleton space, while the contrastive method aligns the features generated by both tasks within the feature space. We further present an innovative view augmentation method that enhances the model's capacity to understand human interaction relationships by shuffling and replacing data across temporal, spatial, and personal dimensions. Extensive experiments on four downstream tasks across three large-scale datasets demonstrate that DDC exhibits stronger representational capabilities compared to state-of-the-art methods.

## Key Ideas
1.We propose the Dual-Decoder method. Specifically, when combining two tasks, we build on the Autoencoder structure that connects the skeletal data at both ends and use a Dual-Decoder as a bridge to combine two tasks at both the skeleton data space and the feature space. This approach involves an additional alignment using contrastive learning in the feature space, extending beyond the skeleton data space.

2.We propose the Multi-dimensional Hybrid Generation based on Normal Augmentations (MHGNA) method, which, in addition to utilizing temporal and spatial information, emphasizes the interactions between individuals.
![MHGNA](https://github.com/user-attachments/assets/358997dd-4843-4dd2-885b-b73b4140dbb2)


## Requirements
```bash
python==3.8.13
torch==1.9.0+cu111
```

## Data Preparation
1.Please refer to [skeleton-contrast](https://github.com/fmthoker/skeleton-contrast) to download and preprocess the dataset

2.Organize the files as follows:
```
project_root/
├── data/
│   ├── NTU60/
│   │   ├── xsub/
│   │   │   ├── train_data_joint.npy
│   │   │   ├── train_label.pkl
│   │   │   ├── train_num_frame.npy
│   │   │   ├── val_data_joint.npy
│   │   │   ├── val_label.pkl
│   │   │   └── val_num_frame.npy
│   │   └── xview/
│   │       └── ...
│   ├── NTU120/
│   │   ├── xsub/
│   │   │   └── ...
│   │   └── xset/
│   │       └── ...
│   └── ...
└── README.md
```
## Training and Testing
Train on NTU Datasets:
```
CUDA_VISIBLE_DEVICES=0 python pretrain_moco_mask.py --lr 0.02 --batch-size 128 --teacher-t 0.05 --student-t 0.1 --topk 16384 --mlp --contrast-t 0.07 --contrast-k 16384 --checkpoint-path checkpoints/DDC_pretrain --schedule 351 --epochs 451 --pre-dataset ntu60 --skeleton-representation graph-based --protocol cross_subject --exp-descri forward_CCD

```
Test on linear action classification:
```
CUDA_VISIBLE_DEVICES=2 python action_classification.py   --lr 0.1 --pretrained checkpoints/DDC_pretrain/checkpoint_0450.pth.tar --savepath checkpoints/DDC_linear_eval --finetune-dataset ntu60 --protocol cross_subject --finetune-skeleton-representation graph-based
```
## Pretrained Models
NTU-60 and NTU-120: [[here]](https://drive.google.com/drive/folders/1GUPFMOaLBHjnrbNeohsH1-BmQPwPqV8R?usp=drive_link).


## Main Results
![image](https://github.com/user-attachments/assets/3238919a-da71-4435-a507-834f78dd1436)

## Acknowledgment
The framework of our code is based on [skeleton-contrast](https://github.com/fmthoker/skeleton-contrast) and [PCM3](https://github.com/JHang2020/PCM3/tree/master?tab=readme-ov-file).

