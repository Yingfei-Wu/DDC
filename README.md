# Dual-Decoder Collaborative Learning with Multi-Hybrid View Augmentation for Self-Supervised 3D Action Recognition
The manuscript has been submitted to Pattern Recognition.

## Abstract
Self-supervised methods, including contrastive learning and masked skeleton modeling, have demonstrated considerable potential in the field of skeleton-based action recognition. While contrastive learning captures fine-grained details at the instance level, masked skeleton modeling emphasizes joint-level features, recent studies have begun to combine these two approaches. However, existing combination methods primarily focus on integrating the tasks within the skeleton space. Moreover, existing contrastive learning methods often fail to exploit the comprehensive interaction information in skeletal structures, resulting in suboptimal performance when recognizing actions involving multiple individuals. To overcome these limitations, we introduce the Dual-Decoder Collaborative Learning with Multi-Hybrid View Augmentation (DDC) method, which connects these two tasks across multiple spaces. Specifically, the masked skeleton modeling task provides diverse views for the contrastive learning task in the skeleton space, while the contrastive method aligns the features generated by both tasks within the feature space. We further present an innovative view augmentation method that enhances the model's capacity to understand human interaction relationships by shuffling and replacing data across temporal, spatial, and personal dimensions. Extensive experiments on four downstream tasks across three large-scale datasets demonstrate that DDC exhibits stronger representational capabilities compared to state-of-the-art methods.

## Requirements
```bash
python==3.8.13
torch==1.9.0+cu111
```

## Data Preparation
1.Please refer to [skeleton-contrast](https://github.com/fmthoker/skeleton-contrast) to download and preprocess the dataset

2.Organize the files as follows:
```
project_root/
├── data/
│   ├── NTU60/
│   │   ├── xsub/
│   │   │   ├── train_data_joint.npy
│   │   │   ├── train_label.pkl
│   │   │   ├── train_num_frame.npy
│   │   │   ├── val_data_joint.npy
│   │   │   ├── val_label.pkl
│   │   │   └── val_num_frame.npy
│   │   └── xview/
│   │       └── ...
│   ├── NTU120/
│   │   ├── xsub/
│   │   │   └── ...
│   │   └── xset/
│   │       └── ...
│   └── ...
└── README.md
```
## Training and Testing
Train on NTU Datasets:
```
CUDA_VISIBLE_DEVICES=0 python pretrain_moco_mask.py --lr 0.02 --batch-size 128 --teacher-t 0.05 --student-t 0.1 --topk 16384 --mlp --contrast-t 0.07 --contrast-k 16384 --checkpoint-path checkpoints/DDC_pretrain --schedule 351 --epochs 451 --pre-dataset ntu60 --skeleton-representation graph-based --protocol cross_subject --exp-descri forward_CCD

```
Test on linear action classification:
```
CUDA_VISIBLE_DEVICES=0 python pretrain_moco_mask.py --lr 0.02 --batch-size 128 --teacher-t 0.05 --student-t 0.1 --topk 16384 --mlp --contrast-t 0.07 --contrast-k 16384 --checkpoint-path checkpoints/pcm3/ntu60_xsub_motion/pretrain --schedule 351 --epochs 451 --pre-dataset ntu60 --skeleton-representation graph-based --protocol cross_subject --exp-descri forward_CCD

```
